#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Neural Cleanse Result Analyzer (Step 2: MAD Outlier Detection)

This script implements the second step of the Neural Cleanse defense:
Analyzing the triggers generated by 'run_neural_cleanse.py'.

It loads all the 'mask' images from a specified results directory,
calculates the L1 norm (sparsity) of each mask, and then uses a
robust statistical method (Median Absolute Deviation, or MAD)
to detect if any of the masks are statistical outliers (i.e.,
have an L1 norm that is *abnormally small*).
"""

import os
import sys
import time
import argparse
import numpy as np
from PIL import Image

def get_dataset_params(dataset_name):
    """Returns params for CIFAR-10 or GTSRB."""
    if dataset_name == 'cifar10':
        num_classes = 10
        input_shape = (32, 32, 3) # (H, W, C)
    elif dataset_name == 'gtsrb':
        num_classes = 43
        input_shape = (32, 32, 3) # (H, W, C)
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}")
    return num_classes, input_shape

def outlier_detection(l1_norm_list, idx_mapping, anomaly_threshold):
    """
    Performs outlier detection using the Median Absolute Deviation (MAD) method.
    """
    consistency_constant = 1.4826  # for normal distribution
    median = np.median(l1_norm_list)
    mad = consistency_constant * np.median(np.abs(l1_norm_list - median))
    
    min_l1_norm = np.min(l1_norm_list)
    # Calculate the anomaly index for the smallest L1 norm
    min_mad_score = np.abs(min_l1_norm - median) / mad if mad > 1e-9 else 0

    print(f'Median L1 Norm: {median:.4f}')
    print(f'MAD: {mad:.4f}')
    print(f'Overall Anomaly Index (for min L1): {min_mad_score:.4f}')

    flag_list = []
    # Identify all potential outliers (those with small L1 norms)
    for y_label in idx_mapping:
        current_l1 = l1_norm_list[idx_mapping[y_label]]
        
        # We only care about outliers at the *small* end
        if current_l1 > median:
            continue
            
        mad_score = np.abs(current_l1 - median) / mad if mad > 1e-9 else 0
        
        if mad_score > anomaly_threshold:
            flag_list.append((y_label, current_l1, mad_score))

    if len(flag_list) > 0:
        flag_list = sorted(flag_list, key=lambda x: x[1]) # Sort by L1 norm

    print('\n--- Potential Backdoor Targets (Outliers) ---')
    print(f"Found {len(flag_list)} potential target(s) with Anomaly Score > {anomaly_threshold}:")
    print(f"{'Target Class':<15} | {'L1 Norm':<15} | {'Anomaly Score':<15}")
    print("-"*50)
    for y_label, l_norm, score in flag_list:
        print(f"{y_label:<15} | {l_norm:<15.4f} | {score:<15.4f}")
    
    return min_mad_score, flag_list

def analyze_pattern_norm_dist(args):
    """
    Loads all mask images from the results directory, calculates their
    L1 norms, and calls the outlier detection function.
    """
    
    num_classes, input_shape = get_dataset_params(args.dataset)
    
    mask_flatten_list = []
    idx_mapping = {} # Maps class_label -> index in mask_flatten_list
    
    # Reconstruct the filename template to match the generation script
    img_filename_template = f"{args.dataset}_resnet18_visualize_%s_label_%d.png"

    print(f"Scanning for masks in: {args.result_dir}")
    print(f"Using filename template: {img_filename_template % ('mask', 0)}")

    for y_label in range(num_classes):
        mask_filename = img_filename_template % ('mask', y_label)
        full_mask_path = os.path.join(args.result_dir, mask_filename)
        
        if os.path.isfile(full_mask_path):
            try:
                img = Image.open(full_mask_path).convert('L') # 'L' for grayscale
                
                # Ensure image is resized to the expected input shape
                if img.size != (input_shape[1], input_shape[0]): # PIL.Image.size is (W, H)
                     img = img.resize((input_shape[1], input_shape[0]), Image.Resampling.LANCZOS)
                
                mask = np.array(img, dtype='float32') # (H, W)
                mask /= 255.0 # Normalize from [0, 255] to [0, 1]

                mask_flatten_list.append(mask.flatten()) # Flatten to 1D array
                idx_mapping[y_label] = len(mask_flatten_list) - 1
                
            except Exception as e:
                print(f"Warning: Could not load or process mask for label {y_label} at {full_mask_path}: {e}")
                continue

    l1_norm_list = [np.sum(np.abs(m)) for m in mask_flatten_list]

    if len(l1_norm_list) == 0:
        print(f"Error: No mask files found in '{args.result_dir}' matching the template.")
        print("Please run 'run_neural_cleanse.py' first.")
        return

    print(f'Successfully loaded and processed {len(l1_norm_list)} masks.')

    outlier_detection(l1_norm_list, idx_mapping, args.anomaly_threshold)


def main(args):
    print(f'{sys.argv[0]} start')
    start_time = time.time()
    analyze_pattern_norm_dist(args)
    elapsed_time = time.time() - start_time
    print(f'Elapsed time: {elapsed_time:.2f} s')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Analyze Neural Cleanse results using MAD outlier detection.")
    
    # --- Essential Arguments ---
    parser.add_argument('--result_dir', type=str, required=True, 
                        help="Directory containing the mask/pattern images generated by run_neural_cleanse.py.")
    parser.add_argument('--dataset', type=str, required=True, 
                        choices=['cifar10', 'gtsrb'],
                        help="Dataset name (must match the one used for generation).")
    
    # --- Analysis Parameters ---
    parser.add_argument('--anomaly_threshold', type=float, default=2.0,
                        help="MAD score threshold to flag a class as an outlier (default: 2.0).")

    args = parser.parse_args()
    main(args)